{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjyWRHiq8o0foKql53SY/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aniket21628/AI_Support/blob/main/Pokemon_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uw4UebY4gVGs"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color = \"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ],
      "metadata": {
        "id": "ZTqT_AHHhaYZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('NVIDIA_API_KEY')"
      ],
      "metadata": {
        "id": "hIxcxNwgh9IC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from functools import partial\n",
        "\n",
        "def RPrint(preface = \"State: \"):\n",
        "  def print_and_return(x, preface=\"\"):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "  return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def PPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        pprint(preface, x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ],
      "metadata": {
        "id": "2tu5ZSwKiMii"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-community wikipedia"
      ],
      "metadata": {
        "id": "_zIGpxmfixN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import json\n",
        "\n",
        "docs = WikipediaLoader(query=\"Pokemon and Everything about Pokemon\", load_max_docs=1000).load()\n"
      ],
      "metadata": {
        "id": "FDMzqdzBi2mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chunks_with_headers(doc, doc_index):\n",
        "    chunk_size = 800\n",
        "    chunk_overlap = 100\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    doc_content = doc.page_content\n",
        "    doc_length = len(doc.page_content)\n",
        "\n",
        "    while start < doc_length:\n",
        "        end = min(start + chunk_size, doc_length)\n",
        "        chunk = doc_content[start:end]\n",
        "\n",
        "        if start != 0:\n",
        "            chunk = doc_content[max(start - chunk_overlap, 0):end]\n",
        "\n",
        "        chunk_json = {\n",
        "            \"meta_data\": {\n",
        "                \"title\": doc.metadata[\"title\"],\n",
        "                \"summary\": doc.metadata['summary'],\n",
        "                \"source_url\": doc.metadata['source'],\n",
        "            },\n",
        "            \"chunk_index\": len(chunks) + 1,\n",
        "            \"content\": chunk\n",
        "        }\n",
        "        chunks.append(chunk_json)\n",
        "\n",
        "        start += chunk_size\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "yzG066s3lJw-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunks = []\n",
        "\n",
        "for i,doc in enumerate(docs):\n",
        "  chunks = create_chunks_with_headers(doc, i+1)\n",
        "  all_chunks.extend(chunks)\n",
        "  print(f\"Created {len(chunks)} chunks for document {i+1}\")\n",
        "\n",
        "print(\"All data has been processed\")"
      ],
      "metadata": {
        "id": "933b2UuYrC46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet lark langchain-chroma\n",
        "%pip install --upgrade --quiet langchain-cohere"
      ],
      "metadata": {
        "id": "DBOzAvNsrrxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "# Get API key\n",
        "cohere_api_key = userdata.get('COHERE_API_KEY')\n",
        "\n",
        "# Initialize embeddings with model\n",
        "embeddings_model = CohereEmbeddings(\n",
        "    cohere_api_key=cohere_api_key,\n",
        "    model=\"embed-english-v3.0\"\n",
        ")\n",
        "\n",
        "# Convert chunks to Document objects\n",
        "def convert_chunks_to_documents(chunks):\n",
        "    return [\n",
        "        Document(page_content=chunk[\"content\"], metadata=chunk[\"meta_data\"])\n",
        "        for chunk in chunks\n",
        "    ]\n",
        "\n",
        "# Batching and backoff wrapper\n",
        "def embed_documents_in_batches(documents, batch_size=25, delay=65):\n",
        "    vectorstores = []\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i + batch_size]\n",
        "        print(f\"Embedding batch {i // batch_size + 1}/{(len(documents) + batch_size - 1) // batch_size}\")\n",
        "        try:\n",
        "            vs = Chroma.from_documents(batch, embeddings_model)\n",
        "            vectorstores.append(vs)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed batch {i // batch_size + 1}: {e}\")\n",
        "            print(\"Waiting before retry...\")\n",
        "            time.sleep(delay)\n",
        "    return vectorstores\n",
        "\n",
        "# === Main processing ===\n",
        "all_chunks = []\n",
        "\n",
        "# Example: Suppose `docs` is your list of Wikipedia documents\n",
        "for i, doc in enumerate(docs):\n",
        "    chunks = create_chunks_with_headers(doc, i)\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "documents = convert_chunks_to_documents(all_chunks)\n",
        "\n",
        "# Now embed in safe batches\n",
        "vectorstores = embed_documents_in_batches(documents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdRW-1rJsYP5",
        "outputId": "859567f6-f197-431d-82f3-f196df6888fd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding batch 1/90\n",
            "Embedding batch 2/90\n",
            "Embedding batch 3/90\n",
            "Embedding batch 4/90\n",
            "Embedding batch 5/90\n",
            "Embedding batch 6/90\n",
            "Embedding batch 7/90\n",
            "Embedding batch 8/90\n",
            "Embedding batch 9/90\n",
            "Embedding batch 10/90\n",
            "Embedding batch 11/90\n",
            "Embedding batch 12/90\n",
            "Embedding batch 13/90\n",
            "Embedding batch 14/90\n",
            "Embedding batch 15/90\n",
            "Embedding batch 16/90\n",
            "Embedding batch 17/90\n",
            "Embedding batch 18/90\n",
            "Embedding batch 19/90\n",
            "Embedding batch 20/90\n",
            "Embedding batch 21/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding batch 22/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed batch 22: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}\n",
            "Waiting before retry...\n",
            "Embedding batch 23/90\n",
            "Embedding batch 24/90\n",
            "Embedding batch 25/90\n",
            "Embedding batch 26/90\n",
            "Embedding batch 27/90\n",
            "Embedding batch 28/90\n",
            "Embedding batch 29/90\n",
            "Embedding batch 30/90\n",
            "Embedding batch 31/90\n",
            "Embedding batch 32/90\n",
            "Embedding batch 33/90\n",
            "Embedding batch 34/90\n",
            "Embedding batch 35/90\n",
            "Embedding batch 36/90\n",
            "Embedding batch 37/90\n",
            "Embedding batch 38/90\n",
            "Embedding batch 39/90\n",
            "Embedding batch 40/90\n",
            "Embedding batch 41/90\n",
            "Embedding batch 42/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding batch 43/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed batch 43: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}\n",
            "Waiting before retry...\n",
            "Embedding batch 44/90\n",
            "Embedding batch 45/90\n",
            "Embedding batch 46/90\n",
            "Embedding batch 47/90\n",
            "Embedding batch 48/90\n",
            "Embedding batch 49/90\n",
            "Embedding batch 50/90\n",
            "Embedding batch 51/90\n",
            "Embedding batch 52/90\n",
            "Embedding batch 53/90\n",
            "Embedding batch 54/90\n",
            "Embedding batch 55/90\n",
            "Embedding batch 56/90\n",
            "Embedding batch 57/90\n",
            "Embedding batch 58/90\n",
            "Embedding batch 59/90\n",
            "Embedding batch 60/90\n",
            "Embedding batch 61/90\n",
            "Embedding batch 62/90\n",
            "Embedding batch 63/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding batch 64/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed batch 64: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}\n",
            "Waiting before retry...\n",
            "Embedding batch 65/90\n",
            "Embedding batch 66/90\n",
            "Embedding batch 67/90\n",
            "Embedding batch 68/90\n",
            "Embedding batch 69/90\n",
            "Embedding batch 70/90\n",
            "Embedding batch 71/90\n",
            "Embedding batch 72/90\n",
            "Embedding batch 73/90\n",
            "Embedding batch 74/90\n",
            "Embedding batch 75/90\n",
            "Embedding batch 76/90\n",
            "Embedding batch 77/90\n",
            "Embedding batch 78/90\n",
            "Embedding batch 79/90\n",
            "Embedding batch 80/90\n",
            "Embedding batch 81/90\n",
            "Embedding batch 82/90\n",
            "Embedding batch 83/90\n",
            "Embedding batch 84/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding batch 85/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_cohere.utils:Retrying langchain_cohere.embeddings.CohereEmbeddings.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised TooManyRequestsError: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed batch 85: status_code: 429, body: {'message': 'trial token rate limit exceeded, limit is 100000 tokens per minute'}\n",
            "Waiting before retry...\n",
            "Embedding batch 86/90\n",
            "Embedding batch 87/90\n",
            "Embedding batch 88/90\n",
            "Embedding batch 89/90\n",
            "Embedding batch 90/90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwTEYXtg1qUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}